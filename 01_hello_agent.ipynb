{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9554887d",
   "metadata": {},
   "source": [
    "## Install ```openai-agents``` SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7258532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\My Learning\\GIAIC\\agents-sdk-crash-course\\.venv\\Scripts\\python.exe: No module named pip\n"
     ]
    }
   ],
   "source": [
    "pip install openai-agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61445860",
   "metadata": {},
   "source": [
    "## Make the notebook capable of running the asynchronous functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdd825e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b3eb32",
   "metadata": {},
   "source": [
    "# Run Google Gemini with OpenAI-Agents SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd230388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel, RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7783bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gemini_api_key = os.environ.get(\"GOOGLE_API_KEY\")\n",
    "if not gemini_api_key:\n",
    "    raise ValueError(\"Please set the GOOGLE_API_KEY environment variable with your Google Gemini API key.\")\n",
    "\n",
    "externalClient = AsyncOpenAI(\n",
    "    api_key = gemini_api_key,\n",
    "    base_url = \"https://generativelanguage.googleapis.com/v1beta/openai\", \n",
    ")\n",
    "\n",
    "model = OpenAIChatCompletionsModel(\n",
    "    openai_client = externalClient,\n",
    "    model = \"gemini-2.5-flash\",\n",
    ")\n",
    "\n",
    "# Run level configuration\n",
    "config = RunConfig(\n",
    "    model = model,\n",
    "    model_provider=externalClient,\n",
    "    tracing_disabled=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1056a877",
   "metadata": {},
   "source": [
    "## Method 1: Hello World "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61185077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.run import Agent, Runner, RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0de436e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! The Founder of Pakistan is **Muhammad Ali Jinnah**.\n",
      "\n",
      "He is widely revered in Pakistan as Quaid-e-Azam (Great Leader) and Baba-e-Qaum (Father of the Nation).\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\n",
    "    name = \"simple agent\",\n",
    "    instructions = \"You are a helpful assistant.\",\n",
    "    model = model\n",
    ")\n",
    "\n",
    "# synchronous run\n",
    "result = Runner.run_sync(agent,\"Hello, Who is the Founder of Pakistan?\", run_config=config)\n",
    "\n",
    "print(result.final_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd5c05",
   "metadata": {},
   "source": [
    "## Method 2: Hello World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f4535be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner, RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63bb241e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mountains touch the sky,\n",
      "Indus flows through ancient lands,\n",
      "Spirit strong and free.\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    agent = Agent(\n",
    "        name = \"simple agent\",\n",
    "        instructions = \"You only response in haikus.\",\n",
    "        model = model\n",
    "    )\n",
    "\n",
    "    # asynchronous run\n",
    "    result = await Runner.run(agent,\"tell me about Pakistan?\", run_config=config)\n",
    "\n",
    "    print(result.final_output)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0143fc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AgentUpdatedStreamEvent(new_agent=Agent(name='simple agent', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='You only response in haikus.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001CFA34C6C40>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), type='agent_updated_stream_event')\n",
      "RawResponsesStreamEvent(data=ResponseCreatedEvent(response=Response(id='__fake_id__', created_at=1754602074.3759286, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=0, type='response.created'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemAddedEvent(item=ResponseOutputMessage(id='__fake_id__', content=[], role='assistant', status='in_progress', type='message'), output_index=0, sequence_number=1, type='response.output_item.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartAddedEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='', type='output_text', logprobs=None), sequence_number=2, type='response.content_part.added'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseTextDeltaEvent(content_index=0, delta='High peaks touch the sky,\\nIndus whispers ancient tales,\\nWarm hearts greet you there.', item_id='__fake_id__', logprobs=[], output_index=0, sequence_number=3, type='response.output_text.delta'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseContentPartDoneEvent(content_index=0, item_id='__fake_id__', output_index=0, part=ResponseOutputText(annotations=[], text='High peaks touch the sky,\\nIndus whispers ancient tales,\\nWarm hearts greet you there.', type='output_text', logprobs=None), sequence_number=4, type='response.content_part.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseOutputItemDoneEvent(item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='High peaks touch the sky,\\nIndus whispers ancient tales,\\nWarm hearts greet you there.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), output_index=0, sequence_number=5, type='response.output_item.done'), type='raw_response_event')\n",
      "RawResponsesStreamEvent(data=ResponseCompletedEvent(response=Response(id='__fake_id__', created_at=1754602074.3759286, error=None, incomplete_details=None, instructions=None, metadata=None, model='gemini-2.5-flash', object='response', output=[ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='High peaks touch the sky,\\nIndus whispers ancient tales,\\nWarm hearts greet you there.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=False, temperature=None, tool_choice='auto', tools=[], top_p=None, background=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=None, safety_identifier=None, service_tier=None, status=None, text=None, top_logprobs=None, truncation=None, usage=None, user=None), sequence_number=6, type='response.completed'), type='raw_response_event')\n",
      "RunItemStreamEvent(name='message_output_created', item=MessageOutputItem(agent=Agent(name='simple agent', handoff_description=None, tools=[], mcp_servers=[], mcp_config={}, instructions='You only response in haikus.', prompt=None, handoffs=[], model=<agents.models.openai_chatcompletions.OpenAIChatCompletionsModel object at 0x000001CFA34C6C40>, model_settings=ModelSettings(temperature=None, top_p=None, frequency_penalty=None, presence_penalty=None, tool_choice=None, parallel_tool_calls=None, truncation=None, max_tokens=None, reasoning=None, metadata=None, store=None, include_usage=None, response_include=None, extra_query=None, extra_body=None, extra_headers=None, extra_args=None), input_guardrails=[], output_guardrails=[], output_type=None, hooks=None, tool_use_behavior='run_llm_again', reset_tool_choice=True), raw_item=ResponseOutputMessage(id='__fake_id__', content=[ResponseOutputText(annotations=[], text='High peaks touch the sky,\\nIndus whispers ancient tales,\\nWarm hearts greet you there.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message'), type='message_output_item'), type='run_item_stream_event')\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    agent = Agent(\n",
    "        name = \"simple agent\",\n",
    "        instructions = \"You only response in haikus.\",\n",
    "        model = model\n",
    "    )\n",
    "\n",
    "    # asynchronous streamed run\n",
    "    result = Runner.run_streamed(agent,\"tell me about Pakistan?\", run_config=config)\n",
    "\n",
    "    async for chunk in result.stream_events():\n",
    "        print(chunk)\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ae960",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents-sdk-crash-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
